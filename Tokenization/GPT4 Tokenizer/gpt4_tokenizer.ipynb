{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "Build your own GPT-4 Tokenizer!\n",
    "\n",
    "### Step 1\n",
    "\n",
    "Write the `BasicTokenizer` class, with the following three core functions:\n",
    "\n",
    "- `def train(self, text, vocab_size, verbose=False)`\n",
    "- `def encode(self, text)`\n",
    "- `def decode(self, ids)`\n",
    "\n",
    "Train your tokenizer on whatever text you like and visualize the merged tokens. Do they look reasonable? One default test you may wish to use is the text file `tests/taylorswift.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from itertools import tee\n",
    "import regex as re\n",
    "\n",
    "class BasicTokenizer():\n",
    "\n",
    "    def __init__(self, byte_permutation=None) -> None:\n",
    "        \"\"\"\n",
    "        - byte_permutation: mapping of bytes (values from 0 to 255) that may be used for encoding purposes\n",
    "        - special_tokens: str -> int dictionary of special tokens\n",
    "          example: {'<|endoftext|>': 100257}\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.vocab = None\n",
    "        self.merges = []  \n",
    "        self.byte_permutation = byte_permutation\n",
    "        self.special_tokens = {}\n",
    "        \n",
    "\n",
    "    def train(self, text, vocab_size, verbose=False, flag=False):\n",
    "        \"\"\"\n",
    "        Train the tokenizer using Byte Pair Encoding (BPE).\n",
    "\n",
    "        Args:\n",
    "            text (str): The input text to train on.\n",
    "            vocab_size (int): The desired vocabulary size.\n",
    "            verbose (bool, optional): If True, print merge operations. Defaults to False.\n",
    "            flag (bool, optional): If True, train based on the gpt4 regex pattern. Defaults to False.\n",
    "        \"\"\"\n",
    "\n",
    "        # Step 1: Construct the base vocabulary\n",
    "        self.vocab = set(self.get_vocab(text=text).keys())        \n",
    "        if flag: \n",
    "            # Split text using the regex pattern\n",
    "            tokens = re.findall(GPT4_SPLIT_PATTERN, text)\n",
    "   \n",
    "        tokens = [' '.join(list(word)) + ' </w>' for word in text.split()]\n",
    "\n",
    "        # Iterate (steps 2-4) until a specified number of iterations are reached \n",
    "        while len(self.vocab) < vocab_size:\n",
    "            \n",
    "            # Step 2: Find most frequent pair\n",
    "            pairs = self.get_stats(tokens)\n",
    "            if not pairs:\n",
    "                break\n",
    "            max_freq_pair = self.get_best_pair(pairs)\n",
    "            \n",
    "            # Step 3: Merge pair\n",
    "            tokens = self.merge_vocab(max_freq_pair, tokens)\n",
    "\n",
    "            # Step 4: Update vocabulary \n",
    "            self.vocab.add(''.join(max_freq_pair))\n",
    "            self.merges.append(max_freq_pair)\n",
    "\n",
    "            if verbose: \n",
    "                print(f\"Merge: {max_freq_pair}\")\n",
    "\n",
    "    def encode(self, text, flag):\n",
    "        \"\"\"Encode the input text into a sequence of tokens.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input text to encode.\n",
    "            flag (bool, optional): If True, train based on the gpt4 regex pattern. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            list of str: List of encoded tokens.\n",
    "        \"\"\"\n",
    "        if flag: \n",
    "            # Split text using the regex pattern\n",
    "            tokens = re.findall(GPT4_SPLIT_PATTERN, text)\n",
    "        # Initial tokenization\n",
    "        tokens = [' '.join(list(word)) + ' </w>' for word in text.split()]\n",
    "        \n",
    "        # Applying BPE merges\n",
    "        for pair in self.merges:\n",
    "            tokens = self.merge_vocab(pair, tokens)\n",
    "        \n",
    "        # Token flattening\n",
    "        encoded_text = [word.split() for word in tokens]\n",
    "        encoded_ids = [token for sublist in encoded_text for token in sublist]\n",
    "        \n",
    "        if flag:\n",
    "            # Apply byte permutation\n",
    "            # Apply byte permutation only to single-character tokens\n",
    "            permuted_ids = []\n",
    "            for token in encoded_ids:\n",
    "                if len(token) == 1:\n",
    "                    byte_val = ord(token)\n",
    "                    if 0 <= byte_val <= 255:\n",
    "                        permuted_ids.append(self.byte_permutation[byte_val])\n",
    "                    else:\n",
    "                        permuted_ids.append(token)  # Keep non-byte characters as they are\n",
    "                else:\n",
    "                    permuted_ids.append(token)\n",
    "            return permuted_ids\n",
    "\n",
    "        return encoded_ids\n",
    "\n",
    "    def decode(self, ids, flag):\n",
    "        \"\"\"Decode a sequence of tokens back into the original text.\n",
    "\n",
    "        Args:\n",
    "            ids (list of str): List of token IDs to decode.\n",
    "\n",
    "        Returns:\n",
    "            str: The decoded text.\n",
    "        \"\"\"\n",
    "        if flag:\n",
    "            inv_map = {v: k for k, v in self.byte_permutation.items()}\n",
    "            reversed_ids = [chr(inv_map[id]) if isinstance(id, int) else id for id in ids]\n",
    "            decoded_text = ''.join(reversed_ids).replace('</w>', ' ')\n",
    "        else:\n",
    "            decoded_text = ''.join(reversed_ids).replace(' </w>', '')\n",
    "        return decoded_text.strip()\n",
    "\n",
    "    def define_special_tokens(self, special_tokens):\n",
    "        self.special_tokens = special_tokens\n",
    "        self.vocab = self.vocab.add(self.special_tokens)\n",
    "\n",
    "    def get_vocab(self, text):\n",
    "        # Initialize vocabulary with frequency of each word in text\n",
    "        base_vocab = Counter(text)\n",
    "        return {word: freq for word, freq in base_vocab.items()}\n",
    "    \n",
    "    def get_set_chars(self, text):\n",
    "        # Initialize vocabulary with the set of characters in text\n",
    "        base_vocab = set()\n",
    "        for byte_arr in text: \n",
    "            for byte in byte_arr:\n",
    "                base_vocab.add(byte)\n",
    "        return base_vocab\n",
    "    \n",
    "    def pairwise(self, iterable):\n",
    "        #\"s -> (s0,s1), (s1,s2), (s2, s3), ...\"\n",
    "        a, b = tee(iterable)\n",
    "        next(b, None)\n",
    "        return zip(a, b)\n",
    "    \n",
    "    def get_freq_of_pairs(self, text):\n",
    "        pair_words = list(self.pairwise(text.split()))\n",
    "        freq_pairs = Counter(pair_words)\n",
    "        return  {tuple_: freq for tuple_, freq in freq_pairs.items()}\n",
    "        \n",
    "    def get_best_pair(self, pairs):\n",
    "        # Get the most frequent pair\n",
    "        best_pair = max(pairs, key=pairs.get)\n",
    "        return best_pair\n",
    "        \n",
    "    def get_stats(self, tokens):\n",
    "        \"\"\"Compute the frequency of each pair of symbols in the tokenized text.\n",
    "\n",
    "        Args:\n",
    "            tokens (list of str): List of tokens to analyze.\n",
    "\n",
    "        Returns:\n",
    "            defaultdict: Dictionary with pairs of symbols as keys and their frequency as values.\n",
    "        \"\"\"\n",
    "        pairs = defaultdict(int)\n",
    "        for word in tokens:\n",
    "            symbols = word.split()\n",
    "            for i in range(len(symbols) - 1):\n",
    "                pairs[(symbols[i], symbols[i + 1])] += 1\n",
    "        return pairs\n",
    "\n",
    "    def merge_vocab(self, pair, tokens):\n",
    "        \"\"\"Merge the most frequent pair of symbols in the tokens.\n",
    "\n",
    "        Args:\n",
    "            pair (tuple of str): The pair of symbols to merge.\n",
    "            tokens (list of str): List of tokens to process.\n",
    "\n",
    "        Returns:\n",
    "            list of str: New list of tokens with the pair merged.\n",
    "        \"\"\"\n",
    "        bigram = re.escape(' '.join(pair))\n",
    "        pattern = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "        new_tokens = [pattern.sub(''.join(pair), word) for word in tokens]\n",
    "        return new_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The absolute path of the given file is:  c:\\Users\\c.manara\\Documents\\GitHub\\Hands-on-LLMs-NLP-Transformers-Training\\Tokenization\\GPT4 Tokenizer\\taylorswift.txt\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "import os\n",
    "\n",
    "def open_file(path: str) -> List[str]:\n",
    "    try:\n",
    "        with open(file=path, mode=\"r\", encoding=\"UTF-8\") as file: \n",
    "            content = file.read()\n",
    "    except FileNotFoundError:\n",
    "        print(\"File not found!\")\n",
    "    return content\n",
    "\n",
    "\n",
    "file_name = \"taylorswift.txt\"\n",
    "cur_dir = os.getcwd()\n",
    "abs_path = os.path.join(cur_dir, file_name)\n",
    "print(\"The absolute path of the given file is: \", abs_path)\n",
    "\n",
    "text = open_file(path=abs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge: ('.', '</w>')\n",
      "Merge: ('e', '</w>')\n",
      "Merge: (',', '</w>')\n",
      "Merge: ('d', '</w>')\n",
      "Merge: ('r', '</w>')\n",
      "Merge: ('2', '0')\n",
      "Merge: ('s', '</w>')\n",
      "Merge: ('i', 'n')\n",
      "Merge: ('t', '</w>')\n",
      "Merge: ('o', 'n')\n",
      "Merge: ('r', 'i')\n",
      "Merge: ('e', 'd</w>')\n",
      "Merge: ('t', 'h')\n",
      "Merge: ('a', 'n')\n",
      "Merge: ('e', 'r</w>')\n",
      "Merge: ('a', 'r')\n",
      "Merge: ('y', '</w>')\n",
      "Merge: ('a', 'l')\n",
      "Merge: ('th', 'e</w>')\n",
      "Merge: ('v', 'ed</w>')\n",
      "Merge: ('w', 'i')\n",
      "Merge: ('20', '1')\n",
      "Merge: ('e', 'r')\n",
      "Merge: ('on', '</w>')\n",
      "Merge: ('wi', 'f')\n",
      "Merge: ('R', 'e')\n",
      "Merge: ('S', 'wif')\n",
      "Merge: ('o', 'r</w>')\n",
      "Merge: ('c', 'h')\n",
      "Merge: ('o', 'm')\n",
      "Merge: ('20', '2')\n",
      "Merge: ('b', 'er</w>')\n",
      "Merge: ('a', 'y')\n",
      "Merge: ('e', 'n')\n",
      "Merge: ('o', 'r')\n",
      "Merge: ('al', '</w>')\n",
      "Merge: ('e', 'm')\n",
      "Merge: ('ri', 'e')\n",
      "Merge: ('in', 'g')\n",
      "Merge: ('t', 'i')\n",
      "Merge: ('ay', 'l')\n",
      "Merge: ('\"', '.</w>')\n",
      "Merge: ('l', 'l')\n",
      "Merge: ('T', 'ayl')\n",
      "Merge: ('t', 'rie')\n",
      "Merge: ('t', 'o')\n",
      "Merge: ('Re', 'trie')\n",
      "Merge: ('Retrie', 'ved</w>')\n",
      "Merge: ('Tayl', 'or</w>')\n",
      "Merge: ('e', 's')\n",
      "Merge: ('Swif', 't</w>')\n",
      "Merge: ('u', 's')\n",
      "Merge: (']', '</w>')\n",
      "Merge: ('om', '</w>')\n",
      "Merge: (')', '.</w>')\n",
      "Merge: ('em', 'ber</w>')\n",
      "Merge: ('r', 'om</w>')\n",
      "Merge: ('A', 'r')\n",
      "Merge: ('f', 'rom</w>')\n",
      "Merge: ('an', 'd</w>')\n",
      "Merge: ('ing', '</w>')\n",
      "Merge: ('r', 'e')\n",
      "Merge: ('o', 'u')\n",
      "Merge: ('o', 'ri')\n",
      "Merge: ('o', 'f')\n",
      "Merge: ('g', 'in')\n",
      "Merge: ('ch', 'i')\n",
      "Merge: ('gin', 'al</w>')\n",
      "Merge: ('ori', 'ginal</w>')\n",
      "Merge: ('h', 'e</w>')\n",
      "Merge: ('Ar', 'chi')\n",
      "Merge: ('Archi', 'ved</w>')\n",
      "Merge: ('a', '</w>')\n",
      "Merge: ('of', '</w>')\n",
      "Merge: ('s', 't')\n",
      "Merge: ('i', 'c')\n",
      "Merge: ('.', '[')\n",
      "Merge: ('e', 'c')\n",
      "Merge: ('i', 'll')\n",
      "Merge: (\"'\", 's</w>')\n",
      "Merge: ('\"', 'Taylor</w>')\n",
      "Merge: ('o', 'v')\n",
      "Merge: ('a', 't')\n",
      "Merge: ('in', '</w>')\n",
      "Merge: ('e', 's</w>')\n",
      "Merge: ('202', '3')\n",
      "Merge: ('a', 's</w>')\n",
      "Merge: ('to', '</w>')\n",
      "Merge: ('J', 'u')\n",
      "Merge: ('Swif', 't')\n",
      "Merge: ('th', '</w>')\n",
      "Merge: ('u', 'm')\n",
      "Merge: ('T', 'he</w>')\n",
      "Merge: ('ar', 'd')\n",
      "Merge: ('an', '</w>')\n",
      "Merge: ('e', 'l')\n",
      "Merge: ('ar', 'y</w>')\n",
      "Merge: ('a', 'm')\n",
      "Merge: ('1', ',</w>')\n",
      "Merge: ('2', '.</w>')\n",
      "Merge: ('l', 'y</w>')\n",
      "Merge: ('o', 'p')\n",
      "Merge: ('t', 'r')\n",
      "Merge: ('i', 's')\n",
      "Merge: ('h', 'er</w>')\n",
      "Merge: ('o', '</w>')\n",
      "Merge: ('u', 'ary</w>')\n",
      "Merge: (':', '</w>')\n",
      "Merge: ('N', 'ov')\n",
      "Merge: ('us', 'ic')\n",
      "Merge: ('Nov', 'ember</w>')\n",
      "Merge: ('e', 'w')\n",
      "Merge: ('l', '</w>')\n",
      "Merge: ('a', 't</w>')\n",
      "Merge: ('b', 'o')\n",
      "Merge: ('D', 'ec')\n",
      "Merge: ('i', 't')\n",
      "Merge: ('i', 'g')\n",
      "Merge: ('B', 'ill')\n",
      "Merge: ('a', 's')\n",
      "Merge: ('3', ',</w>')\n",
      "Merge: ('on', 'g')\n",
      "Merge: ('O', 'c')\n",
      "Merge: ('a', 'ti')\n",
      "Merge: ('S', 't')\n",
      "Merge: ('Oc', 'to')\n",
      "Merge: ('Octo', 'ber</w>')\n",
      "Merge: ('a', 'c')\n",
      "Merge: ('o', 'w')\n",
      "Merge: ('Dec', 'ember</w>')\n",
      "Merge: ('Bill', 'bo')\n",
      "Merge: ('u', 'r')\n",
      "Merge: ('a', 'd')\n",
      "Merge: ('l', 'e')\n",
      "Merge: ('f', 'or</w>')\n",
      "Merge: ('2', ',</w>')\n",
      "Merge: ('ch', '</w>')\n",
      "Merge: ('u', 'g')\n",
      "Merge: ('1', '0')\n",
      "Merge: ('s', 't</w>')\n",
      "Merge: ('k', '</w>')\n",
      "Merge: ('20', '20')\n",
      "Merge: ('ou', 'n')\n",
      "Merge: ('2023', '.</w>')\n",
      "Merge: ('9', ',</w>')\n",
      "Merge: ('202', '1')\n",
      "Merge: ('b', 'um')\n",
      "Merge: ('o', 'l')\n",
      "Merge: ('20', '0')\n",
      "Merge: ('us', 't</w>')\n",
      "Merge: ('e', 'b')\n",
      "Merge: ('M', 'a')\n",
      "Merge: ('Ju', 'ly</w>')\n",
      "Merge: ('8', ',</w>')\n",
      "Merge: ('4', ',</w>')\n",
      "Merge: ('7', ',</w>')\n",
      "Merge: ('er', 's')\n",
      "Merge: (']', '[')\n",
      "Merge: ('A', 'ug')\n",
      "Merge: ('Swift', \"'s</w>\")\n",
      "Merge: ('Aug', 'ust</w>')\n",
      "Merge: ('i', 'd')\n",
      "Merge: ('m', 'e')\n",
      "Merge: ('e', 'p')\n",
      "Merge: ('6', ',</w>')\n",
      "Merge: ('n', '</w>')\n",
      "Merge: ('201', '9')\n",
      "Merge: ('wi', 'th</w>')\n",
      "Merge: ('e', 'ar')\n",
      "Merge: ('f', 'i')\n",
      "Merge: ('n', 'e</w>')\n",
      "Merge: ('5', ',</w>')\n",
      "Merge: ('Billbo', 'ard')\n",
      "Merge: ('usic', '</w>')\n",
      "Merge: ('ri', 't')\n",
      "Merge: ('h', 'i')\n",
      "Merge: ('202', '2.</w>')\n",
      "Merge: ('en', '</w>')\n",
      "Merge: ('N', 'ew')\n",
      "Merge: ('d', 'i')\n",
      "Merge: ('A', 'p')\n",
      "Merge: (\"'\", '</w>')\n",
      "Merge: ('r', 'o')\n",
      "Merge: ('s', ',</w>')\n",
      "Merge: ('Ju', 'ne</w>')\n",
      "Merge: ('t', 's</w>')\n",
      "Merge: ('c', 'or')\n",
      "Merge: ('201', '5')\n",
      "Merge: ('201', '6')\n",
      "Merge: ('2021', '.</w>')\n",
      "Merge: ('i', 'm')\n",
      "Merge: ('eb', 'r')\n",
      "Merge: ('o', 'll')\n",
      "Merge: ('M', 'ar')\n",
      "Merge: ('ri', 'c')\n",
      "Merge: ('Billboard', '.</w>')\n",
      "Merge: (',', '[')\n",
      "Merge: ('F', 'ebr')\n",
      "Merge: ('Febr', 'uary</w>')\n",
      "Merge: ('\"', '</w>')\n",
      "Merge: ('e', 'a')\n",
      "Merge: ('201', '2.</w>')\n",
      "Merge: ('l', 'e</w>')\n",
      "Merge: ('ep', 't')\n",
      "Merge: ('Ma', 'y</w>')\n",
      "Merge: ('Ap', 'ri')\n",
      "Merge: ('Apri', 'l</w>')\n",
      "Merge: ('A', 'w')\n",
      "Merge: ('S', 'ept')\n",
      "Merge: ('Sept', 'ember</w>')\n",
      "Merge: ('r', 'a')\n",
      "Merge: ('al', 'bum')\n",
      "Merge: ('ati', 'on')\n",
      "Merge: ('v', 'e</w>')\n",
      "Merge: ('C', 'h')\n",
      "Merge: ('es', 't</w>')\n",
      "Merge: ('J', 'an')\n",
      "Merge: ('Jan', 'uary</w>')\n",
      "Merge: ('oun', 'tr')\n",
      "Merge: ('ig', 'h')\n",
      "Merge: ('A', 'l')\n",
      "Merge: ('Mar', 'ch</w>')\n",
      "Merge: ('2016', '.</w>')\n",
      "Merge: ('w', 'rit')\n",
      "Merge: ('s', 'e')\n",
      "Merge: ('2020', '.</w>')\n",
      "Merge: ('l', 'o')\n",
      "Merge: ('s', 'ong')\n",
      "Merge: ('ar', 'd</w>')\n",
      "Merge: ('u', 'l')\n",
      "Merge: ('en', 't')\n",
      "Merge: ('i', 's</w>')\n",
      "Merge: ('ti', 'c')\n",
      "Merge: ('a', 'g')\n",
      "Merge: ('f', 'or')\n",
      "Merge: ('e', '.</w>')\n",
      "Merge: ('A', 'n')\n",
      "Merge: ('2023', ').</w>')\n",
      "Merge: ('ountr', 'y</w>')\n",
      "Merge: ('2015', '.</w>')\n",
      "Merge: ('A', 'me')\n",
      "Merge: ('Ame', 'ric')\n",
      "Merge: ('u', 't')\n",
      "Merge: ('s', 'ing')\n",
      "Merge: ('op', '</w>')\n",
      "Merge: ('e', 't')\n",
      "Merge: ('201', '7')\n",
      "Merge: ('w', 'as</w>')\n",
      "Merge: ('I', 'n')\n",
      "Merge: ('2019', '.</w>')\n",
      "Merge: ('o', 'c')\n",
      "Merge: ('0', '</w>')\n",
      "Merge: (')', '</w>')\n",
      "Merge: ('M', 'usic</w>')\n",
      "Merge: ('(', 'November</w>')\n",
      "Merge: ('l', 'i')\n",
      "Merge: ('New', '</w>')\n",
      "Merge: ('v', 'i')\n",
      "Merge: ('Aw', 'ard')\n",
      "Merge: ('S', 'he</w>')\n",
      "Merge: ('t', 'er')\n",
      "Merge: ('e', ',</w>')\n",
      "Merge: ('b', 'y</w>')\n",
      "Merge: ('s', 'he</w>')\n",
      "Merge: ('2', ').</w>')\n",
      "Merge: ('a', 'i')\n",
      "Merge: ('p', '</w>')\n",
      "Merge: ('2', '1,</w>')\n",
      "Merge: ('a', 'b')\n",
      "Merge: ('y', ',</w>')\n",
      "Merge: ('cor', 'd')\n",
      "Merge: ('S', 'p')\n",
      "Merge: ('1', '3,</w>')\n",
      "Merge: ('an', 'd')\n",
      "Merge: ('.[', '1')\n",
      "Merge: ('ou', 'r')\n",
      "Merge: ('u', 'n')\n",
      "Merge: ('o', 't')\n",
      "Merge: ('Americ', 'an</w>')\n",
      "Merge: ('(', 'October</w>')\n",
      "Merge: ('s', 'u')\n",
      "Merge: ('o', 'g')\n",
      "Merge: ('10', ',</w>')\n",
      "Merge: ('W', 'i')\n",
      "Merge: ('re', 'c')\n",
      "Merge: ('es', 's')\n",
      "Merge: ('en', 't</w>')\n",
      "Merge: ('S', 'ong')\n",
      "Merge: ('ow', '</w>')\n",
      "Merge: ('201', '4')\n",
      "Merge: ('y', '.</w>')\n",
      "Merge: ('T', 'im')\n",
      "Merge: ('St', 'on')\n",
      "Merge: ('e', 'i')\n",
      "Merge: ('ou', 'r</w>')\n",
      "Merge: ('r', 'st</w>')\n",
      "Merge: ('o', 'd')\n",
      "Merge: ('.[', '2')\n",
      "Merge: ('1', '8,</w>')\n",
      "Merge: ('i', 'on')\n",
      "Merge: ('R', 'oll')\n",
      "Merge: ('3', '0')\n",
      "Merge: ('ar', 't')\n",
      "Merge: ('e', 'll')\n",
      "Merge: ('er', ',</w>')\n",
      "Merge: ('at', 'ed</w>')\n",
      "Merge: ('2', '3,</w>')\n",
      "Merge: ('id', 'e')\n",
      "Merge: ('for', 'm')\n",
      "Merge: ('an', 'c')\n",
      "Merge: ('Roll', 'ing</w>')\n",
      "Merge: ('e', 'v')\n",
      "Merge: ('h', 'a')\n",
      "Merge: ('\"', ',</w>')\n",
      "Merge: ('c', 'on')\n",
      "Merge: ('s', 's')\n",
      "Merge: ('e', 'd')\n",
      "Merge: ('1', '1,</w>')\n",
      "Merge: ('201', '3')\n",
      "Merge: ('en', 'n')\n",
      "Merge: ('a', 'in')\n",
      "Merge: ('2', '4,</w>')\n",
      "Merge: ('p', 'r')\n",
      "Merge: ('Al', 'bum')\n",
      "Merge: ('ar', 'ti')\n",
      "Merge: ('G', 'r')\n",
      "Merge: ('s', '\".</w>')\n",
      "Merge: ('ati', 'on</w>')\n",
      "Merge: ('i', 'on</w>')\n",
      "Merge: ('4', ']</w>')\n",
      "Merge: ('.[', '5')\n",
      "Merge: ('2', '2,</w>')\n",
      "Merge: ('201', '8')\n",
      "Merge: ('1', '9')\n",
      "Merge: ('201', '0')\n",
      "Merge: ('en', 'd')\n",
      "Merge: ('i', 'st')\n",
      "Merge: ('202', '4')\n",
      "Merge: ('p', 'l')\n",
      "Merge: ('es', 't')\n",
      "Merge: ('1', '2,</w>')\n",
      "Merge: ('e', '\".</w>')\n",
      "Merge: ('es', 's</w>')\n",
      "Merge: ('on', 'e</w>')\n",
      "Merge: ('T', 'h')\n",
      "Merge: ('s', '.</w>')\n",
      "Merge: ('u', 'c')\n",
      "Merge: ('i', 'an')\n",
      "Merge: ('e', 'x')\n",
      "Merge: ('e', 'e')\n",
      "Merge: ('er', 's</w>')\n",
      "Merge: ('7', ']</w>')\n",
      "Merge: ('1', '9,</w>')\n",
      "Merge: ('Billbo', 'ard</w>')\n",
      "Merge: ('.[', '3')\n",
      "Merge: ('.[', '4')\n",
      "Merge: ('2', ']</w>')\n",
      "Merge: ('200', '9')\n",
      "Merge: ('1', '4,</w>')\n",
      "Merge: ('I', 's</w>')\n",
      "Merge: ('c', 'om')\n",
      "Merge: ('h', 'as</w>')\n",
      "Merge: ('m', 'y</w>')\n",
      "Merge: ('8', ']</w>')\n",
      "Merge: ('th', 'at</w>')\n",
      "Merge: ('I', 'n</w>')\n",
      "Merge: ('20', ',</w>')\n",
      "Merge: ('(', 'December</w>')\n",
      "Merge: ('o', 't</w>')\n",
      "Merge: ('ea', 'r</w>')\n",
      "Merge: ('3', ']</w>')\n",
      "Merge: ('5', ']</w>')\n",
      "Merge: ('1', '7,</w>')\n",
      "Merge: ('s', 'p')\n",
      "Merge: ('A', '</w>')\n",
      "Merge: ('l', 'u')\n",
      "Merge: ('m', 'usic</w>')\n",
      "Merge: ('W', 'h')\n",
      "Merge: ('2', '7,</w>')\n",
      "Merge: ('2', '6,</w>')\n",
      "Merge: ('2017', '.</w>')\n",
      "Merge: ('202', '2).</w>')\n",
      "Merge: ('fi', 'rst</w>')\n",
      "Merge: ('p', 'p')\n",
      "Merge: ('t', 'er</w>')\n",
      "Merge: ('S', 'h')\n",
      "Merge: ('or', 'l')\n",
      "Merge: ('h', '</w>')\n",
      "Merge: ('ti', 'on')\n",
      "Merge: ('9', ']</w>')\n",
      "Merge: ('J', 'o')\n",
      "Merge: ('Ston', 'e.</w>')\n",
      "Merge: ('L', 'i')\n",
      "Merge: ('6', ']</w>')\n",
      "Merge: ('a', 'k')\n",
      "Merge: ('ll', '</w>')\n",
      "Merge: ('vi', 'ew')\n",
      "Merge: ('2', '9,</w>')\n",
      "Merge: ('p', 'h')\n",
      "Merge: ('M', 'T')\n",
      "Merge: ('m', 'o')\n",
      "Merge: ('Re', 'cord')\n",
      "Merge: ('album', '</w>')\n",
      "Merge: ('i', 'r')\n",
      "Merge: ('\"', 'The</w>')\n",
      "Merge: ('en', 'c')\n",
      "Merge: ('1', ']</w>')\n",
      "Merge: ('es', '.</w>')\n",
      "Merge: ('a', ',</w>')\n",
      "Merge: ('U', 'S')\n",
      "Merge: ('as', 'h')\n",
      "Merge: ('H', 'er</w>')\n",
      "Merge: ('le', 'as')\n",
      "Merge: ('m', 'an')\n",
      "Merge: ('Wi', 'th</w>')\n",
      "Merge: ('Ar', 'ti')\n",
      "Merge: ('c', 'ountry</w>')\n",
      "Merge: ('m', 'ill')\n",
      "Merge: (';', '</w>')\n",
      "Merge: ('1', '5,</w>')\n",
      "Merge: ('a', 'r</w>')\n",
      "Merge: ('Re', 'p')\n",
      "Merge: ('0', '0')\n",
      "Merge: ('C', 'ountry</w>')\n",
      "Merge: ('201', '1')\n",
      "Merge: ('C', 'on')\n",
      "Merge: ('ide', 'o</w>')\n",
      "Merge: ('M', 'usic')\n",
      "Merge: ('e', 'g')\n",
      "Merge: ('ov', 'er</w>')\n",
      "Merge: ('or', 'e</w>')\n",
      "Merge: ('m', 'usic')\n",
      "Merge: ('Y', 'or')\n",
      "Merge: ('c', 'ri')\n",
      "Merge: (\"'\", '\".</w>')\n",
      "Merge: ('30', ',</w>')\n",
      "Merge: ('v', 'ill')\n",
      "Merge: ('w', 'hi')\n",
      "Merge: ('2020', ').</w>')\n",
      "Merge: ('m', '</w>')\n",
      "Merge: ('th', 'er</w>')\n",
      "Merge: ('p', 'er')\n",
      "Merge: ('][', '3')\n",
      "Merge: ('2', '5,</w>')\n",
      "Merge: ('1', '</w>')\n",
      "Merge: ('Swift', '\".</w>')\n",
      "Merge: ('i', 'l')\n",
      "Merge: ('a', 'y</w>')\n",
      "Merge: ('H', 'ot</w>')\n",
      "Merge: ('f', 'f')\n",
      "Merge: ('ee', 'k')\n",
      "Merge: ('q', 'u')\n",
      "Merge: ('(', 'July</w>')\n",
      "Merge: ('2013', '.</w>')\n",
      "Merge: ('1', '6,</w>')\n",
      "Merge: ('song', 'writ')\n",
      "Merge: ('u', 'b')\n",
      "Merge: ('b', 'ec')\n",
      "Merge: ('re', 'leas')\n",
      "Merge: ('Y', 'ou')\n",
      "Merge: ('E', 'r')\n",
      "Merge: ('T', 'our</w>')\n",
      "Merge: ('w', 'h')\n",
      "Merge: ('Song', 'writ')\n",
      "Merge: ('er', 'e</w>')\n",
      "Merge: ('e', 'y</w>')\n",
      "Merge: ('2019', ').</w>')\n",
      "Merge: ('ar', 't</w>')\n",
      "Merge: ('(', 'March</w>')\n",
      "Merge: ('is', 't</w>')\n",
      "Merge: ('re', 'am')\n",
      "Merge: ('es', ',</w>')\n",
      "Merge: ('n', 'um')\n",
      "Merge: ('b', 'er')\n",
      "Merge: ('v', 'er')\n",
      "Merge: ('G', 're')\n",
      "Merge: ('in', 'c')\n",
      "Merge: ('whi', 'ch</w>')\n",
      "Merge: ('d', ',</w>')\n",
      "Merge: ('R', 'o')\n",
      "Merge: ('Swift', ':</w>')\n",
      "Merge: ('a', 'st')\n",
      "Merge: ('N', 'ash')\n",
      "Merge: ('Nash', 'vill')\n",
      "Merge: ('p', 'op</w>')\n",
      "Merge: ('Gr', 'am')\n",
      "Merge: ('Album', '</w>')\n",
      "Merge: ('a', 'v')\n",
      "Merge: ('M', 'c')\n",
      "Merge: ('al', 'l')\n",
      "Merge: ('G', 'u')\n",
      "Merge: ('(', 'August</w>')\n",
      "Merge: ('Swift', ',</w>')\n",
      "Merge: ('E', 'n')\n",
      "Merge: ('at', 'e</w>')\n",
      "Merge: ('19', '8')\n",
      "Merge: ('Y', 'ear')\n",
      "Merge: ('t', ',</w>')\n",
      "Merge: ('k', 'e</w>')\n",
      "Merge: ('o', 'o')\n",
      "Merge: ('Tayl', 'or')\n",
      "Merge: ('am', 'e</w>')\n",
      "Merge: ('2024', '.</w>')\n",
      "Merge: ('t', 'ur')\n",
      "Merge: ('or', 'y</w>')\n",
      "Merge: ('200', '8')\n",
      "Merge: ('ac', 'k</w>')\n",
      "Merge: ('2021', ').</w>')\n",
      "Merge: ('lo', 'b')\n",
      "Merge: ('Y', 'ear</w>')\n",
      "Merge: ('ti', 'm')\n",
      "Merge: ('al', 'e</w>')\n",
      "Merge: ('W', 'orl')\n",
      "Merge: ('Gram', 'my</w>')\n",
      "Merge: ('cor', 'd</w>')\n",
      "Merge: ('Ch', 'ri')\n",
      "Merge: ('ou', 't</w>')\n",
      "Merge: ('s', 'h')\n",
      "Merge: ('B', 'est</w>')\n",
      "Merge: ('2', '8,</w>')\n",
      "Merge: ('ra', 'ph')\n",
      "Merge: ('198', '9')\n",
      "Merge: ('an', 'i')\n",
      "Merge: ('ic', '</w>')\n",
      "Merge: ('us', 'tr')\n",
      "Merge: ('Er', 'as</w>')\n",
      "Merge: ('i', 'v')\n",
      "Merge: ('New', 's.</w>')\n",
      "Merge: ('p', 'e')\n",
      "Merge: ('y', 'l')\n",
      "Merge: ('B', 'ig')\n",
      "Merge: ('est', '-')\n",
      "Merge: ('al', 'l</w>')\n",
      "Merge: ('e', \"'s</w>\")\n",
      "Merge: ('i', 'me')\n",
      "Merge: ('u', 'al</w>')\n",
      "Merge: ('o', '.</w>')\n",
      "Merge: ('A', 'c')\n",
      "Merge: ('ew', '</w>')\n",
      "Merge: ('Rep', 'ut')\n",
      "Merge: ('ad', 'e</w>')\n",
      "Merge: ('Taylor', \"'s</w>\")\n",
      "Merge: ('c', 'er')\n",
      "Merge: ('0', ']</w>')\n",
      "Merge: ('][', '5')\n",
      "Merge: ('oc', 'i')\n",
      "Merge: ('in', 'n')\n",
      "Merge: ('(', 'June</w>')\n",
      "Merge: ('3', '1,</w>')\n",
      "Merge: ('T', 'our')\n",
      "Merge: ('s', 'o</w>')\n",
      "Merge: ('p', 'ro')\n",
      "Merge: ('w', 'om')\n",
      "Merge: ('arti', 'st')\n",
      "Merge: ('album', 's</w>')\n",
      "Merge: ('You', '</w>')\n",
      "Merge: ('el', '</w>')\n",
      "Merge: ('W', 'ill')\n",
      "Merge: ('u', 'r</w>')\n",
      "Merge: ('em', 'ale</w>')\n",
      "Merge: ('T', 'ime')\n",
      "Merge: ('F', 'or')\n",
      "Merge: ('o', 'us')\n",
      "Merge: ('ar', 'e</w>')\n",
      "Merge: ('10', '0')\n",
      "Merge: ('an', 'n')\n",
      "Merge: ('ti', 'on</w>')\n",
      "Merge: ('V', 'ideo</w>')\n",
      "Merge: ('ub', 'l')\n",
      "Merge: ('ad', 'i')\n",
      "Merge: ('A', 'll</w>')\n",
      "Merge: ('mill', 'ion</w>')\n",
      "Merge: ('k', 'ed</w>')\n",
      "Merge: ('it', 'y</w>')\n",
      "Merge: ('per', 'form')\n",
      "Merge: ('mo', 'st</w>')\n",
      "Merge: (',', '00')\n",
      "Merge: ('W', 'eek')\n",
      "Merge: ('to', 'p</w>')\n",
      "Merge: ('â€“', '</w>')\n",
      "Merge: ('Tim', 'es.</w>')\n",
      "Merge: ('an', 'g')\n",
      "Merge: ('releas', 'ed</w>')\n",
      "Merge: ('ear', 'l')\n",
      "Merge: ('it', 's</w>')\n",
      "Merge: ('ea', 'k</w>')\n",
      "Merge: ('k', 'l')\n",
      "Merge: ('b', 'e')\n",
      "Merge: ('Arti', 'st</w>')\n",
      "Merge: ('I', '</w>')\n",
      "Merge: ('t', 'e</w>')\n",
      "Merge: ('Aw', 'ard</w>')\n",
      "Merge: ('om', 'in')\n",
      "Merge: ('e', 't</w>')\n",
      "Merge: ('U', 'n')\n",
      "Merge: ('ers', 'ion')\n",
      "Merge: ('2018', '.</w>')\n",
      "Merge: ('(', 'April</w>')\n",
      "Merge: ('(', 'January</w>')\n",
      "Merge: ('man', ',</w>')\n",
      "Merge: ('Award', 's</w>')\n",
      "Merge: ('T', 'enn')\n",
      "Merge: ('Tenn', 'es')\n",
      "Merge: ('Tennes', 'se')\n",
      "Merge: ('s', 'i')\n",
      "Merge: ('o', ',</w>')\n",
      "Merge: ('M', 'e')\n",
      "Merge: ('a', 'u')\n",
      "Merge: ('id', 'n')\n",
      "Merge: ('Yor', 'k</w>')\n",
      "Merge: ('e', 'f')\n",
      "Merge: ('&', '</w>')\n",
      "Merge: ('arti', 'st</w>')\n",
      "Merge: ('cri', 'tic')\n",
      "Merge: ('A', 'ss')\n",
      "Merge: ('(', 'February</w>')\n",
      "Merge: ('N', 'o.</w>')\n",
      "Merge: ('di', 'a</w>')\n",
      "Merge: ('k', 's</w>')\n",
      "Merge: ('e', 'at')\n",
      "Merge: ('in', 'e</w>')\n",
      "Merge: ('ig', 'n')\n",
      "Merge: ('F', 'earl')\n",
      "Merge: ('f', 'ter</w>')\n",
      "Merge: ('h', 'igh')\n",
      "Merge: ('d', 'on')\n",
      "Merge: ('g', 'est</w>')\n",
      "Merge: ('u', 't</w>')\n",
      "Merge: ('g', 'en')\n",
      "Merge: ('w', 'on</w>')\n",
      "Merge: ('V', 'ersion')\n",
      "Merge: ('V', 'a')\n",
      "Merge: ('2014', ').</w>')\n",
      "Merge: ('201', '2).</w>')\n",
      "Merge: ('H', 'ow</w>')\n",
      "Merge: ('re', 'e</w>')\n",
      "Merge: ('d', 'uc')\n",
      "Merge: ('a', 'p')\n",
      "Merge: ('sing', 'le</w>')\n",
      "Merge: ('M', 'idn')\n",
      "Merge: ('Midn', 'igh')\n",
      "Merge: ('][', '2')\n",
      "Merge: ('][', '4')\n",
      "Merge: ('P', 'e')\n",
      "Merge: (',[', '5')\n",
      "Merge: ('anc', 'e</w>')\n",
      "Merge: ('F', 'i')\n",
      "Merge: ('T', 'r')\n",
      "Merge: ('a', 'd</w>')\n",
      "Merge: ('ag', 'e</w>')\n",
      "Merge: ('MT', 'V')\n",
      "Merge: ('S', 'ing')\n",
      "Merge: ('ha', 've</w>')\n",
      "Merge: ('d', 'es')\n",
      "Merge: ('Sp', 'eak</w>')\n",
      "Merge: ('10', '0</w>')\n",
      "Merge: ('t', 'ing</w>')\n",
      "Merge: ('\"', 'B')\n",
      "Merge: ('G', 'lob')\n",
      "Merge: ('For', 'b')\n",
      "Merge: ('M', 'o')\n",
      "Merge: ('lu', 'd')\n",
      "Merge: ('i', 'e</w>')\n",
      "Merge: ('Nashvill', 'e</w>')\n",
      "Merge: ('o', '-')\n",
      "Merge: ('is', 'h')\n",
      "Merge: ('oun', 'd')\n",
      "Merge: (',00', '0</w>')\n",
      "Merge: ('n', 'ew</w>')\n",
      "Merge: ('2017', ').</w>')\n",
      "Merge: ('2014', '.</w>')\n",
      "Merge: ('(', 'September</w>')\n",
      "Merge: ('og', 'raph')\n",
      "Merge: ('al', 'so</w>')\n",
      "Merge: ('t', 'ed</w>')\n",
      "Merge: ('in', 'd')\n",
      "Merge: ('p', 'op')\n",
      "Merge: ('l', 'es</w>')\n",
      "Merge: ('y', 'n')\n",
      "Merge: ('B', 'lo')\n",
      "Merge: ('re', 'cord')\n",
      "Merge: ('r', 'e</w>')\n",
      "Merge: ('inc', 'lud')\n",
      "Merge: (',[', '4')\n",
      "Merge: ('t', 'e')\n",
      "Merge: ('ai', 'd</w>')\n",
      "Merge: ('Ass', 'oci')\n",
      "Merge: ('fi', 'el')\n",
      "Merge: ('as', 's')\n",
      "Merge: ('a', 'ul')\n",
      "Merge: ('Ch', 'art</w>')\n",
      "Merge: ('2009', ').</w>')\n",
      "Merge: ('en', 'ts</w>')\n",
      "Merge: ('1989', '</w>')\n",
      "Merge: ('P', 'op</w>')\n",
      "Merge: ('g', 'u')\n",
      "Merge: ('om', 'e</w>')\n",
      "Merge: ('ong', '</w>')\n",
      "Merge: ('st', 'ream')\n",
      "Merge: ('song', 's</w>')\n",
      "Merge: ('E', 'ver')\n",
      "Merge: ('202', '2')\n",
      "Merge: ('a', 'w')\n",
      "Merge: ('ro', 'ss')\n",
      "Merge: ('r', 'es')\n",
      "Merge: ('C', 'ar')\n",
      "Merge: ('el', 'p')\n",
      "Merge: ('y', 'ear')\n",
      "Merge: ('Chri', 's</w>')\n",
      "Merge: ('m', 'ore</w>')\n",
      "Merge: ('C', 'MT')\n",
      "Merge: ('t', 'ain')\n",
      "Merge: ('ra', 'is')\n",
      "Merge: ('(', \"Taylor's</w>\")\n",
      "Merge: ('rie', 't')\n",
      "Merge: ('2015', ').</w>')\n",
      "Merge: ('y', '\".</w>')\n",
      "Merge: ('fiel', 'd,</w>')\n",
      "Merge: ('s', 'ec')\n",
      "Merge: ('is', 'c')\n",
      "Merge: ('x', '</w>')\n",
      "Merge: ('u', 'di')\n",
      "Merge: ('L', 'ov')\n",
      "Merge: ('N', 'ow</w>')\n",
      "Merge: ('l', 'an')\n",
      "Merge: ('fi', 'l')\n",
      "Merge: ('bec', 'ame</w>')\n",
      "Merge: ('200', '</w>')\n",
      "Merge: ('w', 'or')\n",
      "Merge: ('k', 'ing</w>')\n",
      "Merge: ('song', '</w>')\n",
      "Merge: ('s', 'e</w>')\n",
      "Merge: ('.[', '6')\n",
      "Merge: ('T', 'op</w>')\n",
      "Merge: ('K', 'ei')\n",
      "Merge: ('f', '</w>')\n",
      "Merge: ('o', 'st')\n",
      "Merge: ('Va', 'riet')\n",
      "Merge: ('(', 'May</w>')\n",
      "Merge: ('c', 'ar')\n",
      "Merge: ('US', '</w>')\n",
      "Merge: ('er', '-')\n",
      "Merge: ('pro', 'duc')\n",
      "Merge: ('di', 'rec')\n",
      "Merge: ('er', '.</w>')\n",
      "Merge: ('to', 'ur</w>')\n",
      "Merge: ('am', 'ed</w>')\n",
      "Merge: ('ur', 'ing</w>')\n",
      "Merge: ('f', 'u')\n",
      "Merge: ('M', 'on')\n",
      "Merge: ('t', 'w')\n",
      "Merge: ('ow', 'n')\n",
      "Merge: ('g', 'h')\n",
      "Merge: ('10', '</w>')\n",
      "Merge: ('Kei', 'th</w>')\n",
      "Merge: ('2', '5')\n",
      "Merge: ('Re', 'view')\n",
      "Merge: ('p', '.</w>')\n",
      "Merge: ('2010', ').</w>')\n",
      "Merge: ('2010', '.</w>')\n",
      "Merge: ('Variet', 'y.</w>')\n",
      "Merge: ('2018', ').</w>')\n",
      "Merge: ('Wi', 'k')\n",
      "Merge: ('su', 'b')\n",
      "Merge: ('E', 'x')\n",
      "Merge: ('F', 'ol')\n",
      "Merge: ('In', 'ter')\n",
      "Merge: ('S', 'c')\n",
      "Merge: ('H', 'ar')\n",
      "Merge: ('ou', 's</w>')\n",
      "Merge: ('An', 'g')\n",
      "Merge: ('num', 'ber</w>')\n",
      "Merge: ('ac', 't</w>')\n",
      "Merge: ('An', 'n')\n",
      "Merge: ('\"', 'Wh')\n",
      "Merge: ('Worl', 'd</w>')\n",
      "Merge: ('ay', 's</w>')\n",
      "Merge: ('in', ',</w>')\n",
      "Merge: ('1', '7')\n",
      "Merge: ('a', 'ri')\n",
      "Merge: ('Gu', 'ard')\n",
      "Merge: ('B', 'C')\n",
      "Merge: ('Pe', 'op')\n",
      "Merge: ('â€¢', '</w>')\n",
      "Merge: ('n', 'ot')\n",
      "Merge: ('1', '3')\n",
      "Merge: ('2023', '</w>')\n",
      "Merge: ('udi', 'o</w>')\n",
      "Merge: ('un', 'd')\n",
      "Merge: ('con', 'cer')\n",
      "Merge: ('f', 'emale</w>')\n",
      "Merge: ('ac', 'k')\n",
      "Merge: ('B', 'ri')\n",
      "Merge: ('1', '5')\n",
      "Merge: ('ol', 'd')\n",
      "Merge: ('p', 'or')\n",
      "Merge: ('rec', 'ei')\n",
      "Merge: ('i', 'z')\n",
      "Merge: ('A', 'm')\n",
      "Merge: ('B', 'r')\n",
      "Merge: ('en', ',</w>')\n",
      "Merge: ('P', 'r')\n",
      "Merge: ('le', '.</w>')\n",
      "Merge: ('ian', '.</w>')\n",
      "Merge: ('ers', 'on')\n",
      "Merge: ('MTV', '</w>')\n",
      "Merge: ('Big', '</w>')\n",
      "Merge: ('in', 'f')\n",
      "Merge: ('Fearl', 'ess</w>')\n",
      "Merge: ('R', 'ed</w>')\n",
      "Merge: ('Fol', 'kl')\n",
      "Merge: ('high', 'est-')\n",
      "Merge: ('Record', 'ing</w>')\n",
      "Merge: ('st', '-')\n",
      "Merge: ('a', 'pp')\n",
      "Merge: ('i', 'an</w>')\n",
      "Merge: ('at', 't')\n",
      "Merge: ('ad', 'em')\n",
      "Merge: ('v', 'oc')\n",
      "Merge: ('d', 'e')\n",
      "Merge: ('th', 'an</w>')\n",
      "Merge: ('at', 'es</w>')\n",
      "Merge: ('w', 'ere</w>')\n",
      "Merge: ('B', 'o')\n",
      "Merge: ('st', 'ar')\n",
      "Merge: ('ic', 'k')\n",
      "Merge: ('L', 'o')\n",
      "Merge: ('en', 'ti')\n",
      "Merge: ('a', 'z')\n",
      "Merge: ('am', '</w>')\n",
      "Merge: ('i', 'al</w>')\n",
      "Merge: ('t', 'or')\n",
      "Merge: ('Re', 'cord</w>')\n",
      "Merge: ('D', 'ai')\n",
      "Merge: ('Guard', 'ian.</w>')\n",
      "Merge: ('CMT', '.</w>')\n",
      "Merge: ('\"', 'How</w>')\n",
      "Merge: ('c', 'l')\n",
      "Merge: ('u', 's</w>')\n",
      "Merge: ('ation', 's</w>')\n",
      "Merge: ('us', 'in')\n",
      "Merge: ('ti', 've</w>')\n",
      "Merge: ('gu', 'it')\n",
      "Merge: ('u', 're')\n",
      "Merge: ('g', 'ross')\n",
      "Merge: ('Tim', 'e</w>')\n",
      "Merge: ('Songwrit', 'ers</w>')\n",
      "Merge: ('W', 'om')\n",
      "Merge: ('re', 'cord</w>')\n",
      "Merge: ('i', 't</w>')\n",
      "Merge: ('ec', 't')\n",
      "Merge: ('1', '2')\n",
      "Merge: ('m', 'ent</w>')\n",
      "Merge: ('m', 'ak')\n",
      "Merge: ('ic', 'e</w>')\n",
      "Merge: ('200', '7')\n",
      "Merge: ('t', 'an')\n",
      "Merge: ('C', 'ri')\n",
      "Merge: ('er', 'ed</w>')\n",
      "Merge: (\"'\", 't</w>')\n",
      "Merge: ('um', '</w>')\n",
      "Merge: ('C', 'om')\n",
      "Merge: ('es', '\".</w>')\n",
      "Merge: ('2011', ').</w>')\n",
      "Merge: ('2011', '.</w>')\n",
      "Merge: (')', '\".</w>')\n",
      "Merge: ('2009', '.</w>')\n",
      "Merge: ('Arti', 'st')\n",
      "Merge: ('th', 'er')\n",
      "Merge: ('s', 'hi')\n",
      "Merge: ('sing', 'les</w>')\n",
      "Merge: ('or', 'y')\n",
      "Merge: ('ber', '-')\n",
      "Merge: ('pp', 'or')\n",
      "Merge: ('O', 'ff')\n",
      "Merge: ('Reput', 'ation</w>')\n",
      "Merge: ('or', 'e')\n",
      "Merge: ('on', 'd</w>')\n",
      "Merge: ('w', 'orl')\n",
      "Merge: ('ation', 'al</w>')\n",
      "Merge: ('P', 'h')\n",
      "Merge: ('al', 'es</w>')\n",
      "Merge: ('Sp', 'o')\n",
      "Merge: ('le', 'y</w>')\n",
      "Merge: ('ar', 'k')\n",
      "Merge: ('v', 'e')\n",
      "Merge: ('com', 'm')\n",
      "Merge: ('an', 's')\n",
      "Merge: ('A', 't</w>')\n",
      "Merge: ('of', 'f')\n",
      "Merge: ('ei', 'r</w>')\n",
      "Merge: ('re', 'a')\n",
      "Merge: ('tw', 'o</w>')\n",
      "Merge: ('th', 'r')\n",
      "Merge: ('ers', ',</w>')\n",
      "Merge: ('c', 'o-')\n",
      "Merge: ('Ang', 'el')\n",
      "Merge: ('al', 'ly</w>')\n",
      "Merge: ('ow', 'n</w>')\n",
      "Merge: ('includ', 'ing</w>')\n",
      "Merge: ('or', 'g')\n",
      "Merge: ('a', 'h</w>')\n",
      "Merge: ('En', 'ter')\n",
      "Merge: ('Enter', 'tain')\n",
      "Merge: ('igh', 't</w>')\n",
      "Encoded: ['H', 'ere</w>', 'is</w>', 'my</w>', 'first</w>', 'enc', 'od', 'ing', '!', '</w>']\n",
      "Decoded: Here</w>is</w>my</w>first</w>encoding!\n"
     ]
    }
   ],
   "source": [
    "# Object of the class BasicTokenizer\n",
    "tokenizer = BasicTokenizer()\n",
    "\n",
    "# Train the tokenizer\n",
    "vocab_size = 1000\n",
    "tokenizer.train(text=text, vocab_size=vocab_size, verbose=True, flag=False)\n",
    "\n",
    "# Encode the text\n",
    "encoded_text = tokenizer.encode(\"Here is my first encoding!\", flag=False)\n",
    "print(f\"Encoded: {encoded_text}\")\n",
    "\n",
    "# Decode the text\n",
    "decoded_text = tokenizer.decode(ids=encoded_text, flag=False)\n",
    "print(f\"Decoded: {decoded_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "\n",
    "Convert you `BasicTokenizer` into a `RegexTokenizer`, which takes a regex pattern and splits the text exactly as GPT-4 would. Process the parts separately as before, then concatenate the results. Retrain your tokenizer and compare the results before and after. You should see that you will now have no tokens that go across categories (numbers, letters, punctuation, more than one whitespace). Use the GPT-4 pattern:\n",
    "\n",
    "```\n",
    "GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3\n",
    "\n",
    "You're now ready to load the merges from the GPT-4 tokenizer and show that your tokenizer produces the identical results for both `encode` and `decode`, matching [tiktoken](https://github.com/openai/tiktoken).\n",
    "\n",
    "```\n",
    "# match this\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\") # this is the GPT-4 tokenizer\n",
    "ids = enc.encode(\"hello world!!!? (ì•ˆë…•í•˜ì„¸ìš”!) lol123 ðŸ˜‰\")\n",
    "text = enc.decode(ids) # get the same text back\n",
    "```\n",
    "\n",
    "Unfortunately, you will run into two issues:\n",
    "\n",
    "1. It is not trivial to recover the raw merges from the GPT-4 tokenizer. You can easily recover what we call `vocab` here, and what they call and store under `enc._mergeable_ranks`. Feel free to copy paste the `recover_merges` function in `minbpe/gpt4.py`, which takes these ranks and returns the raw merges. If you wish to know how this function works, read [this](https://github.com/openai/tiktoken/issues/60) and [this](https://github.com/karpathy/minbpe/issues/11#issuecomment-1950805306). Basically, under some conditions it is enough to only store the parent nodes (and their rank) and get rid of the precise details of which children merged up to any parent.\n",
    "2. Second, the GPT-4 tokenizer for some reason permutes its raw bytes. It stores this permutation in the first 256 elements of the mergeable ranks, so you can recover this byte shuffle relatively simply as `byte_shuffle = {i: enc._mergeable_ranks[bytes([i])] for i in range(256)}`. In both your encode and decode, you'll have to shuffle bytes around accordingly. If you're stuck, reference the minbpe/gpt4.py` file for hints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding:  [15339, 1917, 12340, 30, 320, 31495, 230, 75265, 243, 92245, 16715, 28509, 4513, 57037]\n",
      "After decoding:  hello world!!!? (ì•ˆë…•í•˜ì„¸ìš”!) lol123 ðŸ˜‰\n"
     ]
    }
   ],
   "source": [
    "# match this\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\") # this is the GPT-4 tokenizer\n",
    "ids = enc.encode(\"hello world!!!? (ì•ˆë…•í•˜ì„¸ìš”!) lol123 ðŸ˜‰\")\n",
    "text = enc.decode(ids) # get the same text back\n",
    "print(\"Encoding: \", ids)\n",
    "print(\"After decoding: \", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: ['hello</w>', 'world!!!?</w>', '(ì•ˆë…•í•˜ì„¸ìš”!)</w>', 'lol123</w>', 'ðŸ˜‰</w>']\n",
      "Decoded: hello</w>world!!!?</w>(ì•ˆë…•í•˜ì„¸ìš”!)</w>lol123</w>ðŸ˜‰</w>\n"
     ]
    }
   ],
   "source": [
    "# Object of the class BasicTokenizer\n",
    "tokenizer = BasicTokenizer()\n",
    "# Train the tokenizer\n",
    "vocab_size = 1000\n",
    "tokenizer.train(text=text, vocab_size=vocab_size, verbose=False, flag=True)\n",
    "\n",
    "# Encode the text\n",
    "encoded_text = tokenizer.encode(\"hello world!!!? (ì•ˆë…•í•˜ì„¸ìš”!) lol123 ðŸ˜‰\", flag=True)\n",
    "print(f\"Encoded: {encoded_text}\")\n",
    "\n",
    "# Decode the text\n",
    "decoded_text = tokenizer.decode(ids=encoded_text, flag=False)\n",
    "print(f\"Decoded: {decoded_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source of these functions: https://github.com/karpathy/minbpe/blob/master/minbpe/gpt4.py\n",
    "def bpe(mergeable_ranks, token, max_rank):\n",
    "    # helper function used in get_gpt4_merges() to reconstruct the merge forest\n",
    "    parts = [bytes([b]) for b in token]\n",
    "    while True:\n",
    "        min_idx = None\n",
    "        min_rank = None\n",
    "        for i, pair in enumerate(zip(parts[:-1], parts[1:])):\n",
    "            rank = mergeable_ranks.get(pair[0] + pair[1])\n",
    "            if rank is not None and (min_rank is None or rank < min_rank):\n",
    "                min_idx = i\n",
    "                min_rank = rank\n",
    "        if min_rank is None or (max_rank is not None and min_rank >= max_rank):\n",
    "            break\n",
    "        assert min_idx is not None\n",
    "        parts = parts[:min_idx] + [parts[min_idx] + parts[min_idx + 1]] + parts[min_idx + 2:]\n",
    "    return parts\n",
    "\n",
    "def recover_merges(mergeable_ranks):\n",
    "    # the `merges` are already the byte sequences in their merged state.\n",
    "    # so we have to recover the original pairings. We can do this by doing \n",
    "    # a small BPE training run on all the tokens, in their order.\n",
    "    # also see https://github.com/openai/tiktoken/issues/60\n",
    "    # also see https://github.com/karpathy/minbpe/issues/11#issuecomment-1950805306\n",
    "    merges = {}\n",
    "    for token, rank in mergeable_ranks.items():\n",
    "        if len(token) == 1:\n",
    "            continue # skip raw bytes\n",
    "        pair = tuple(bpe(mergeable_ranks, token, max_rank=rank))\n",
    "        assert len(pair) == 2\n",
    "        # recover the integer ranks of the pair\n",
    "        ix0 = mergeable_ranks[pair[0]]\n",
    "        ix1 = mergeable_ranks[pair[1]]\n",
    "        merges[(ix0, ix1)] = rank\n",
    "\n",
    "    return merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: [71, 68, 75, 75, 78, '</w>', 86, 78, 81, 75, 67, 0, 0, 0, 30, '</w>', 7, 'ì•ˆ', 'ë…•', 'í•˜', 'ì„¸', 'ìš”', 0, 8, '</w>', 75, 78, 75, 16, 17, 18, '</w>', 'ðŸ˜‰', '</w>']\n",
      "Decoded: hello world!!!? (ì•ˆë…•í•˜ì„¸ìš”!) lol123 ðŸ˜‰\n"
     ]
    }
   ],
   "source": [
    "# Match tiktoken output\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "mergeable_ranks = enc._mergeable_ranks\n",
    "byte_shuffle = {i: enc._mergeable_ranks[bytes([i])] for i in range(256)}\n",
    "# print(byte_shuffle)\n",
    "\n",
    "# Recover merges and byte permutation\n",
    "merges = recover_merges(mergeable_ranks)\n",
    "# print(\"Recovered merges: \", merges)\n",
    "\n",
    "# Object of the class BasicTokenizer\n",
    "tokenizer = BasicTokenizer(byte_shuffle)\n",
    "\n",
    "# Encode the text\n",
    "encoded_text = tokenizer.encode(\"hello world!!!? (ì•ˆë…•í•˜ì„¸ìš”!) lol123 ðŸ˜‰\", flag=True)\n",
    "print(f\"Encoded: {encoded_text}\")\n",
    "\n",
    "# Decode the text\n",
    "decoded_text = tokenizer.decode(ids=encoded_text, flag=True)\n",
    "print(f\"Decoded: {decoded_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Step 4\n",
    "\n",
    "(Optional, irritating, not obviously useful) Add the ability to handle special tokens. You'll then be able to match the output of tiktoken even when special tokens are present, e.g.:\n",
    "\n",
    "```\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\") # this is the GPT-4 tokenizer\n",
    "ids = enc.encode(\"<|endoftext|>hello world\", allowed_special=\"all\")\n",
    "```\n",
    "\n",
    "Without `allowed_special` tiktoken will error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\") # this is the GPT-4 tokenizer\n",
    "ids = enc.encode(\"<|endoftext|>hello world\", allowed_special=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: [27, 91, 68, 77, 67, 78, 69, 83, 68, 87, 83, 91, 29, 'hello</w>', 'world', '</w>']\n",
      "Decoded: <|endoftext|>hello world\n"
     ]
    }
   ],
   "source": [
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "mergeable_ranks = enc._mergeable_ranks\n",
    "byte_shuffle = {i: enc._mergeable_ranks[bytes([i])] for i in range(256)}\n",
    "\n",
    "special_tokens = \"<|endoftext|>\"\n",
    "tokenizer = BasicTokenizer(byte_shuffle)\n",
    "\n",
    "# Train the tokenizer\n",
    "vocab_size = 1000\n",
    "tokenizer.train(text=text, vocab_size=vocab_size, verbose=False, flag=True)\n",
    "\n",
    "tokenizer.define_special_tokens(special_tokens)\n",
    "\n",
    "# Encode the text\n",
    "encoded_text = tokenizer.encode(\"<|endoftext|>hello world\", flag=True)\n",
    "print(f\"Encoded: {encoded_text}\")\n",
    "\n",
    "# Decode the text\n",
    "decoded_text = tokenizer.decode(ids=encoded_text, flag=True)\n",
    "print(f\"Decoded: {decoded_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5\n",
    "\n",
    "If you've made it this far, you're now a pro at LLM Tokenization! Sadly, you're not exactly done yet because a lot of LLMs outside of OpenAI (e.g. Llama, Mistral) use [sentencepiece](https://github.com/google/sentencepiece) instead. Primary difference being that sentencepiece runs BPE directly on Unicode code points instead of on UTF-8 encoded bytes. Feel free to explore sentencepiece on your own (good luck, it's not too pretty), and stretch goal if you really experience and suffer from the burden of time, re-write your BPE to be on Unicode code points and match the Llama 2 tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['â–Ne', 'w', 'â–', 'Y', 'or', 'k']\n",
      "['â–New', 'â–', 'Y', 'or', 'k']\n",
      "['â–', 'New', 'â–York']\n",
      "['â–New', 'â–York']\n",
      "['â–', 'New', 'â–York']\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "model = spm.SentencePieceTrainer.train(\n",
    "      input=abs_path, \n",
    "      model_prefix='m', \n",
    "      vocab_size=1000, \n",
    "      user_defined_symbols=['<|endoftext|>'])\n",
    "      \n",
    "s = spm.SentencePieceProcessor(model_file='m.model')\n",
    "for n in range(5):\n",
    "      encoded_text = s.encode('New York', out_type=str, enable_sampling=True, alpha=0.1, nbest_size=-1)\n",
    "      print(encoded_text )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
